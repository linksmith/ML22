{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import seaborn as sns\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, \"../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-30 20:03:31.075880: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-30 20:03:31.926318: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from src.data import make_dataset\n",
    "from src.models import imagemodels\n",
    "from src.models import train_model\n",
    "import gin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParsedConfigFileIncludesAndImports(filename='model.gin', imports=['gin.torch.external_configurables'], includes=[])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gin.parse_config_file(\"model.gin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using `gin-config` to easily keep track of our experiments, and to easily save the different things we did during our experiments.\n",
    "\n",
    "The `model.gin` file is a simple file that will try to load parameters for funcitons that are already imported. \n",
    "\n",
    "So, if you wouldnt have imported train_model, the ginfile would not be able to parse settings for train_model.trainloop and will give an error.\n",
    "\n",
    "We can print all the settings that are operational with `gin.operative_config_str()` once we have loaded the functions to memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, while `.get_MNIST()` has two parameters that need to be set (a batchsize and a datadir), we can now load the function without having to do that: gin has done it already for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, test_dataloader = make_dataset.get_MNIST()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import gin.torch.external_configurables\n",
      "\n",
      "# Parameters for get_MNIST:\n",
      "# ==============================================================================\n",
      "get_MNIST.batch_size = 32\n",
      "get_MNIST.data_dir = '../../data/raw'\n",
      "\n",
      "# Parameters for NeuralNetwork:\n",
      "# ==============================================================================\n",
      "NeuralNetwork.num_classes = 10\n",
      "NeuralNetwork.units1 = 512\n",
      "\n",
      "# Parameters for trainloop:\n",
      "# ==============================================================================\n",
      "trainloop.epochs = 10\n",
      "trainloop.learning_rate = 0.001\n",
      "trainloop.log_dir = '../../models/gtest/'\n",
      "trainloop.loss_fn = @CrossEntropyLoss()\n",
      "trainloop.optimizer = @Adam\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(gin.config_str())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A big advantage is that we can save this config as a file; that way it is easy to track what you changed during your experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import metrics\n",
    "accuracy = metrics.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-30 20:03:34.630 | INFO     | src.data.data_tools:dir_add_timestamp:129 - Logging to ../../models/gtest/20230430-2003\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 1875/1875 [00:32<00:00, 57.46it/s]\n",
      "2023-04-30 20:04:09.879 | INFO     | src.models.train_model:trainloop:180 - Epoch 0 train 0.5398 test 0.4946 metric ['0.8313']\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 1875/1875 [00:40<00:00, 46.79it/s]\n",
      "2023-04-30 20:04:52.753 | INFO     | src.models.train_model:trainloop:180 - Epoch 1 train 0.4477 test 0.4607 metric ['0.8372']\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 1875/1875 [00:50<00:00, 36.77it/s]\n",
      "2023-04-30 20:05:46.769 | INFO     | src.models.train_model:trainloop:180 - Epoch 2 train 0.4250 test 0.4833 metric ['0.8298']\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 3/3 [02:12<00:00, 44.04s/it]\n",
      "2023-04-30 20:05:46.777 | INFO     | src.data.data_tools:dir_add_timestamp:129 - Logging to ../../models/gtest/20230430-2005\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 1875/1875 [00:30<00:00, 62.50it/s]\n",
      "2023-04-30 20:06:18.513 | INFO     | src.models.train_model:trainloop:180 - Epoch 0 train 0.4805 test 0.4488 metric ['0.8313']\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 1875/1875 [00:37<00:00, 49.43it/s]\n",
      "2023-04-30 20:06:58.555 | INFO     | src.models.train_model:trainloop:180 - Epoch 1 train 0.3541 test 0.4407 metric ['0.8474']\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 1875/1875 [00:32<00:00, 56.99it/s]\n",
      "2023-04-30 20:07:33.491 | INFO     | src.models.train_model:trainloop:180 - Epoch 2 train 0.3130 test 0.3662 metric ['0.8729']\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 3/3 [01:46<00:00, 35.56s/it]\n",
      "2023-04-30 20:07:33.500 | INFO     | src.data.data_tools:dir_add_timestamp:129 - Logging to ../../models/gtest/20230430-2007\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 1875/1875 [00:34<00:00, 53.77it/s]\n",
      "2023-04-30 20:08:10.539 | INFO     | src.models.train_model:trainloop:180 - Epoch 0 train 0.6145 test 0.4703 metric ['0.8272']\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 1875/1875 [00:33<00:00, 56.46it/s]\n",
      "2023-04-30 20:08:45.651 | INFO     | src.models.train_model:trainloop:180 - Epoch 1 train 0.4158 test 0.4264 metric ['0.8445']\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 1875/1875 [00:33<00:00, 55.89it/s]\n",
      "2023-04-30 20:09:21.096 | INFO     | src.models.train_model:trainloop:180 - Epoch 2 train 0.3766 test 0.4106 metric ['0.8516']\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 3/3 [01:47<00:00, 35.86s/it]\n",
      "2023-04-30 20:09:21.104 | INFO     | src.data.data_tools:dir_add_timestamp:129 - Logging to ../../models/gtest/20230430-2009\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 1875/1875 [00:27<00:00, 67.05it/s]\n",
      "2023-04-30 20:09:51.805 | INFO     | src.models.train_model:trainloop:180 - Epoch 0 train 0.5335 test 0.5266 metric ['0.8174']\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 1875/1875 [00:40<00:00, 46.23it/s]\n",
      "2023-04-30 20:10:35.782 | INFO     | src.models.train_model:trainloop:180 - Epoch 1 train 0.4397 test 0.4675 metric ['0.8366']\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 1875/1875 [00:42<00:00, 44.01it/s]\n",
      "2023-04-30 20:11:21.785 | INFO     | src.models.train_model:trainloop:180 - Epoch 2 train 0.4278 test 0.4567 metric ['0.8338']\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 3/3 [02:00<00:00, 40.22s/it]\n",
      "2023-04-30 20:11:21.794 | INFO     | src.data.data_tools:dir_add_timestamp:129 - Logging to ../../models/gtest/20230430-2011\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 1875/1875 [00:26<00:00, 71.32it/s]\n",
      "2023-04-30 20:11:50.041 | INFO     | src.models.train_model:trainloop:180 - Epoch 0 train 0.4724 test 0.4451 metric ['0.8392']\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 1875/1875 [00:29<00:00, 62.72it/s]\n",
      "2023-04-30 20:12:21.968 | INFO     | src.models.train_model:trainloop:180 - Epoch 1 train 0.3514 test 0.3815 metric ['0.8559']\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 1875/1875 [00:27<00:00, 69.00it/s]\n",
      "2023-04-30 20:12:51.044 | INFO     | src.models.train_model:trainloop:180 - Epoch 2 train 0.3160 test 0.3885 metric ['0.8635']\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 3/3 [01:29<00:00, 29.74s/it]\n",
      "2023-04-30 20:12:51.052 | INFO     | src.data.data_tools:dir_add_timestamp:129 - Logging to ../../models/gtest/20230430-2012\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 1875/1875 [00:26<00:00, 71.59it/s]\n",
      "2023-04-30 20:13:19.520 | INFO     | src.models.train_model:trainloop:180 - Epoch 0 train 0.6417 test 0.4942 metric ['0.8203']\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 1875/1875 [00:29<00:00, 64.61it/s]\n",
      "2023-04-30 20:13:50.509 | INFO     | src.models.train_model:trainloop:180 - Epoch 1 train 0.4253 test 0.4339 metric ['0.8485']\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 1875/1875 [00:26<00:00, 71.86it/s]\n",
      "2023-04-30 20:14:18.342 | INFO     | src.models.train_model:trainloop:180 - Epoch 2 train 0.3845 test 0.3996 metric ['0.8559']\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 3/3 [01:27<00:00, 29.09s/it]\n",
      "2023-04-30 20:14:18.349 | INFO     | src.data.data_tools:dir_add_timestamp:129 - Logging to ../../models/gtest/20230430-2014\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 1875/1875 [00:26<00:00, 70.89it/s]\n",
      "2023-04-30 20:14:47.144 | INFO     | src.models.train_model:trainloop:180 - Epoch 0 train 0.5352 test 0.5065 metric ['0.8299']\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 1875/1875 [00:37<00:00, 49.48it/s]\n",
      "2023-04-30 20:15:27.826 | INFO     | src.models.train_model:trainloop:180 - Epoch 1 train 0.4351 test 0.4702 metric ['0.8282']\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 1875/1875 [00:37<00:00, 49.65it/s]\n",
      "2023-04-30 20:16:08.837 | INFO     | src.models.train_model:trainloop:180 - Epoch 2 train 0.4036 test 0.4426 metric ['0.8393']\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 3/3 [01:50<00:00, 36.82s/it]\n",
      "2023-04-30 20:16:08.848 | INFO     | src.data.data_tools:dir_add_timestamp:129 - Logging to ../../models/gtest/20230430-2016\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 1875/1875 [00:28<00:00, 66.49it/s]\n",
      "2023-04-30 20:16:39.184 | INFO     | src.models.train_model:trainloop:180 - Epoch 0 train 0.4810 test 0.4657 metric ['0.8317']\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 1875/1875 [00:28<00:00, 65.24it/s]\n",
      "2023-04-30 20:17:09.807 | INFO     | src.models.train_model:trainloop:180 - Epoch 1 train 0.3547 test 0.3787 metric ['0.8612']\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 1875/1875 [00:27<00:00, 69.04it/s]\n",
      "2023-04-30 20:17:39.115 | INFO     | src.models.train_model:trainloop:180 - Epoch 2 train 0.3201 test 0.3760 metric ['0.8670']\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 3/3 [01:30<00:00, 30.08s/it]\n",
      "2023-04-30 20:17:39.123 | INFO     | src.data.data_tools:dir_add_timestamp:129 - Logging to ../../models/gtest/20230430-2017\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 1875/1875 [00:28<00:00, 66.61it/s]\n",
      "2023-04-30 20:18:09.346 | INFO     | src.models.train_model:trainloop:180 - Epoch 0 train 0.6612 test 0.4959 metric ['0.8227']\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 1875/1875 [00:27<00:00, 68.49it/s]\n",
      "2023-04-30 20:18:38.512 | INFO     | src.models.train_model:trainloop:180 - Epoch 1 train 0.4405 test 0.4419 metric ['0.8466']\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 1875/1875 [00:28<00:00, 66.50it/s]\n",
      "2023-04-30 20:19:08.570 | INFO     | src.models.train_model:trainloop:180 - Epoch 2 train 0.3979 test 0.4275 metric ['0.8496']\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 3/3 [01:29<00:00, 29.80s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "gin.parse_config_file(\"model.gin\")\n",
    "\n",
    "units = [256, 128, 64]\n",
    "learning_rate = [0.01, 0.001, 0.0001]\n",
    "\n",
    "for unit in units:\n",
    "    for lr in learning_rate:\n",
    "        gin.bind_parameter(\"NeuralNetwork.units2\", unit)\n",
    "        gin.bind_parameter(\"trainloop.learning_rate\", lr)\n",
    "    \n",
    "        model = imagemodels.NeuralNetwork()\n",
    "\n",
    "        model, test_loss =  train_model.trainloop(\n",
    "            epochs=3,\n",
    "            model=model,\n",
    "            metrics=[accuracy],\n",
    "            train_dataloader=train_dataloader,\n",
    "            test_dataloader=test_dataloader,\n",
    "            train_steps=len(train_dataloader),\n",
    "            eval_steps=len(test_dataloader),\n",
    "            tunewriter=[\"tensorboard\", \"gin\"]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the experiment, and study the result with tensorboard. \n",
    "\n",
    "Locally, it is easy to do that with VS code itself. On the server, you have to take these steps:\n",
    "\n",
    "- in the terminal, navigate to ~/code/ML22 \n",
    "- activate the python environment for the shell with `poetry shell`. Note how the correct environment is being activated.\n",
    "- run `tensorboard --logdir=models` in the terminal\n",
    "- tensorboard will launch at `localhost:6006` and vscode will notify you that the port is forwarded\n",
    "- you can either press the `launch` button in VScode or open your local browser at `localhost:6006`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Experiment with things like:\n",
    "\n",
    "- changing the amount of units1 and units2 to values between 16 and 1024. Use factors of 2: 16, 32, 64, etc.\n",
    "- changing the batchsize to values between 4 and 128. Again, use factors of two.\n",
    "- all your experiments are saved in the `models` directory, with a timestamp. Inside you find a saved_config.gin file, that \n",
    "contains all the settings for that experiment. The `events` file is what tensorboard will show.\n",
    "- plot the result in a heatmap: units vs batchsize.\n",
    "- changing the learningrate to values between 1e-2 and 1e-5 \n",
    "- changing the optimizer from SGD to one of the other available algoritms at [torch](https://pytorch.org/docs/stable/optim.html) (scroll down for the algorithms)\n",
    "\n",
    "A note on train_steps: this is a setting that determines how often you get an update. \n",
    "Because our complete dataset is 938 (60000 / 64) batches long, you will need 938 trainstep to cover the complete 60.000 images.\n",
    "\n",
    "This can actually be a bit confusion, because every value below 938 changes the meaning of `epoch` slightly, because one epoch is no longer\n",
    "the full dataset, but simply `trainstep` batches. Setting trainsteps to 100 means you need to wait twice as long before you get feedback on the performance,\n",
    "as compared to trainsteps=50. You will also see that settings trainsteps to 100 improves the learning, but that is simply because the model has seen twice as \n",
    "much examples as compared to trainsteps=50.\n",
    "\n",
    "This implies that it is not usefull to compare trainsteps=50 and trainsteps=100, because setting it to 100 will always be better.\n",
    "Just pick an amount, and adjust your number of epochs accordingly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning-wM7qE7ca-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "45c41bdaf5373703b03bba2d9bd89c97dc8ee5add9f1112e039ff04603b8e2ee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
